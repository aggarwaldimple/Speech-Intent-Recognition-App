{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH+Pc86LHbloovRIh095P4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b879af6fdd0b420c9b9ce748306d0235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dacbf54127474706a5532ae264fb53e3",
              "IPY_MODEL_4315de223491437f836d906637dea165",
              "IPY_MODEL_fd3c90d2979d40339f4006b57ee0dc84"
            ],
            "layout": "IPY_MODEL_93176353a2ae45c7953e71fddaa8460f"
          }
        },
        "dacbf54127474706a5532ae264fb53e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a020591f80dc4066a059be2cb80f7f1c",
            "placeholder": "​",
            "style": "IPY_MODEL_b506bb3fd1504e3cb6b89dde3c909c08",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "4315de223491437f836d906637dea165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f516cb4de3c4fd8bc178e814cefa0ed",
            "max": 212,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b4c21dab773476093eef048e346f5d1",
            "value": 212
          }
        },
        "fd3c90d2979d40339f4006b57ee0dc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_884976b9fd274693b90527efa1065a46",
            "placeholder": "​",
            "style": "IPY_MODEL_19b38266405e4fef9adb1c2b3af0b936",
            "value": " 212/212 [00:00&lt;00:00, 7.65kB/s]"
          }
        },
        "93176353a2ae45c7953e71fddaa8460f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a020591f80dc4066a059be2cb80f7f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b506bb3fd1504e3cb6b89dde3c909c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f516cb4de3c4fd8bc178e814cefa0ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b4c21dab773476093eef048e346f5d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "884976b9fd274693b90527efa1065a46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b38266405e4fef9adb1c2b3af0b936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ce96210fb794f07afff3b1be3d7ac70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b016e29a099442799a527b807ab352f7",
              "IPY_MODEL_aa287b60a4464446a3cd89181113dfe9",
              "IPY_MODEL_bb6a2a38a63243699724dbcb183e7696"
            ],
            "layout": "IPY_MODEL_d76ab41be62d4478b76eb73f63a0403a"
          }
        },
        "b016e29a099442799a527b807ab352f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a462741e84f46699252d33146e01978",
            "placeholder": "​",
            "style": "IPY_MODEL_fb2c471340534987ac7ca4e848db131f",
            "value": "config.json: "
          }
        },
        "aa287b60a4464446a3cd89181113dfe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4546e5ec58504b3aadecfabec660ffb3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_397fb56e038f40c9b1f51cb50f2d0c22",
            "value": 1
          }
        },
        "bb6a2a38a63243699724dbcb183e7696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18418bb230ea4814b028cbe761b6d1eb",
            "placeholder": "​",
            "style": "IPY_MODEL_798de49a3aa04105ac561603084604fd",
            "value": " 1.77k/? [00:00&lt;00:00, 48.9kB/s]"
          }
        },
        "d76ab41be62d4478b76eb73f63a0403a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a462741e84f46699252d33146e01978": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb2c471340534987ac7ca4e848db131f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4546e5ec58504b3aadecfabec660ffb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "397fb56e038f40c9b1f51cb50f2d0c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18418bb230ea4814b028cbe761b6d1eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "798de49a3aa04105ac561603084604fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18c70a02b58d4bbc9efc00d60eb57ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6da2d0f45387444394f3036f1f13d33b",
              "IPY_MODEL_276d298812da49dd861cb4035f472971",
              "IPY_MODEL_4ec7bea99f23459fa9df57cbf1a77edc"
            ],
            "layout": "IPY_MODEL_608eff20b2144f7baa63dd939a80dc20"
          }
        },
        "6da2d0f45387444394f3036f1f13d33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a8595cc81d4822856770c65d02d335",
            "placeholder": "​",
            "style": "IPY_MODEL_7dec2392eae240fc9a395f17546a3a72",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "276d298812da49dd861cb4035f472971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfb71af0a38a45e2aa1ae80d7e050a51",
            "max": 1269737156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70907eb9294743429e621a530246046f",
            "value": 1269737156
          }
        },
        "4ec7bea99f23459fa9df57cbf1a77edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4a6b22294fb44c380052fdecd7414dd",
            "placeholder": "​",
            "style": "IPY_MODEL_47af1d025e7342f79eedaac899b6acbb",
            "value": " 1.27G/1.27G [00:21&lt;00:00, 121MB/s]"
          }
        },
        "608eff20b2144f7baa63dd939a80dc20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a8595cc81d4822856770c65d02d335": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dec2392eae240fc9a395f17546a3a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfb71af0a38a45e2aa1ae80d7e050a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70907eb9294743429e621a530246046f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4a6b22294fb44c380052fdecd7414dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47af1d025e7342f79eedaac899b6acbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aggarwaldimple/Speech-Intent-Recognition-App/blob/main/Speech_to_intent_recognition_using_wav2vec2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torchaudio pydub scikit-learn numpy\n",
        "\n",
        "!pip install transformers==4.41.1 --upgrade --quiet\n",
        "\n",
        "!pip install qdrant-client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVH6K2BJZ4kE",
        "outputId": "cc2bdf8b-c8ee-44f0-874c-dd987aefaf75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchaudio) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchaudio) (3.0.2)\n",
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.12/dist-packages (1.15.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.74.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.11.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple\n",
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from google.colab import files\n",
        "\n",
        "# Audio utils\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import detect_nonsilent\n",
        "\n",
        "# HF model\n",
        "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "TARGET_SR = 16000  # Wav2Vec2 expects 16 kHz\n",
        "MIN_SILENCE_LEN_MS = 150  # consecutive ms of silence to consider as silence\n",
        "SILENCE_DB_OFFSET = 16    # silence threshold = (audio.dBFS - this)\n",
        "\n",
        "# -----------------------------\n",
        "# Model loading (do once)\n",
        "# -----------------------------\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b879af6fdd0b420c9b9ce748306d0235",
            "dacbf54127474706a5532ae264fb53e3",
            "4315de223491437f836d906637dea165",
            "fd3c90d2979d40339f4006b57ee0dc84",
            "93176353a2ae45c7953e71fddaa8460f",
            "a020591f80dc4066a059be2cb80f7f1c",
            "b506bb3fd1504e3cb6b89dde3c909c08",
            "5f516cb4de3c4fd8bc178e814cefa0ed",
            "2b4c21dab773476093eef048e346f5d1",
            "884976b9fd274693b90527efa1065a46",
            "19b38266405e4fef9adb1c2b3af0b936",
            "6ce96210fb794f07afff3b1be3d7ac70",
            "b016e29a099442799a527b807ab352f7",
            "aa287b60a4464446a3cd89181113dfe9",
            "bb6a2a38a63243699724dbcb183e7696",
            "d76ab41be62d4478b76eb73f63a0403a",
            "3a462741e84f46699252d33146e01978",
            "fb2c471340534987ac7ca4e848db131f",
            "4546e5ec58504b3aadecfabec660ffb3",
            "397fb56e038f40c9b1f51cb50f2d0c22",
            "18418bb230ea4814b028cbe761b6d1eb",
            "798de49a3aa04105ac561603084604fd",
            "18c70a02b58d4bbc9efc00d60eb57ad5",
            "6da2d0f45387444394f3036f1f13d33b",
            "276d298812da49dd861cb4035f472971",
            "4ec7bea99f23459fa9df57cbf1a77edc",
            "608eff20b2144f7baa63dd939a80dc20",
            "d1a8595cc81d4822856770c65d02d335",
            "7dec2392eae240fc9a395f17546a3a72",
            "dfb71af0a38a45e2aa1ae80d7e050a51",
            "70907eb9294743429e621a530246046f",
            "c4a6b22294fb44c380052fdecd7414dd",
            "47af1d025e7342f79eedaac899b6acbb"
          ]
        },
        "id": "f5vzPOHjZ8xP",
        "outputId": "a67e627b-1ecf-4fa6-dde3-4cee6f5f51ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b879af6fdd0b420c9b9ce748306d0235"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ce96210fb794f07afff3b1be3d7ac70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18c70a02b58d4bbc9efc00d60eb57ad5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Wav2Vec2Model(\n",
              "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
              "    (conv_layers): ModuleList(\n",
              "      (0): Wav2Vec2LayerNormConvLayer(\n",
              "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
              "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
              "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (feature_projection): Wav2Vec2FeatureProjection(\n",
              "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
              "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
              "      (conv): ParametrizedConv1d(\n",
              "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "        (parametrizations): ModuleDict(\n",
              "          (weight): ParametrizationList(\n",
              "            (0): _WeightNorm()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (padding): Wav2Vec2SamePadLayer()\n",
              "      (activation): GELUActivation()\n",
              "    )\n",
              "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
              "        (attention): Wav2Vec2SdpaAttention(\n",
              "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Wav2Vec2FeedForward(\n",
              "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J5wbPjdAZPnm"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Audio preprocessing\n",
        "# -----------------------------\n",
        "\n",
        "def load_and_preprocess(path: str,\n",
        "                        target_sr: int = TARGET_SR,\n",
        "                        remove_silence: bool = False) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Load an audio file of any common type (mp3/m4a/wav),\n",
        "    convert to mono 16 kHz, optionally remove silence,\n",
        "    and return a 1-D float32 waveform tensor in [-1, 1].\n",
        "    \"\"\"\n",
        "    # 1) Load with pydub (handles many formats)\n",
        "    audio = AudioSegment.from_file(path)\n",
        "\n",
        "    # 2) Standardize sample rate + channels\n",
        "    audio = audio.set_frame_rate(target_sr).set_channels(1)\n",
        "\n",
        "    # 3) Remove silence using energy thresholding\n",
        "    if remove_silence:\n",
        "        # threshold below which audio is considered silent\n",
        "        silence_thresh = audio.dBFS - SILENCE_DB_OFFSET\n",
        "        ranges = detect_nonsilent(audio, min_silence_len=MIN_SILENCE_LEN_MS,\n",
        "                                  silence_thresh=silence_thresh)\n",
        "        if ranges:\n",
        "            trimmed = AudioSegment.silent(duration=0, frame_rate=target_sr)\n",
        "            for start_ms, end_ms in ranges:\n",
        "                trimmed += audio[start_ms:end_ms]\n",
        "            audio = trimmed\n",
        "        # if no ranges found, keep original audio\n",
        "\n",
        "    # 4) Convert to numpy float32 in [-1, 1]\n",
        "    samples = np.array(audio.get_array_of_samples())\n",
        "    # Scale based on sample width (e.g., 16-bit -> 32768)\n",
        "    max_val = float(1 << (8 * audio.sample_width - 1))\n",
        "    samples = (samples.astype(np.float32) / max_val)\n",
        "\n",
        "    # 5) To torch tensor, shape [time]; keep as 1-D for HF extractor\n",
        "    waveform = torch.from_numpy(samples)\n",
        "    return waveform\n",
        "\n",
        "# -----------------------------\n",
        "# Average Embeddings\n",
        "# -----------------------------\n",
        "\n",
        "def average_embeddings(emb_list: List[torch.Tensor], l2_normalize: bool = False) -> torch.Tensor:\n",
        "    \"\"\"Average multiple 1024-d embeddings and (optionally) L2-normalize the result.\"\"\"\n",
        "    if len(emb_list) == 1:\n",
        "        avg = emb_list[0]\n",
        "    else:\n",
        "        avg = torch.stack(emb_list, dim=0).mean(dim=0)\n",
        "    if l2_normalize:\n",
        "        avg = avg / avg.norm(p=2).clamp_min(1e-12)\n",
        "    return avg\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "# Attention pooling layer\n",
        "class AttentionPooling(torch.nn.Module):\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.attention = torch.nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # hidden_states: [seq_len, hidden_size]\n",
        "        attn_scores = self.attention(hidden_states)  # [seq_len, 1]\n",
        "        attn_weights = torch.softmax(attn_scores, dim=0)  # [seq_len, 1]\n",
        "        pooled = torch.sum(attn_weights * hidden_states, dim=0)  # [hidden_size]\n",
        "        return pooled\n",
        "\n",
        "attention_pooling = AttentionPooling(hidden_size=model.config.hidden_size)\n",
        "\n",
        "def audio_to_embedding(wav: torch.Tensor, l2_normalize: bool = True) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Converts waveform to embedding using attention pooling.\n",
        "    \"\"\"\n",
        "    # if wav.ndim > 1:\n",
        "    #   wav = wav.mean(dim=0)\n",
        "    #   wav = wav.squeeze()\n",
        "\n",
        "    if wav.ndim == 2 and wav.shape[0] > 1:\n",
        "        wav = wav.mean(dim=0)  # convert to mono\n",
        "\n",
        "    wav = wav.float()\n",
        "    input_values = feature_extractor(wav, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_values)\n",
        "        hidden_states = outputs.last_hidden_state.squeeze(0)  # [seq_len, hidden_size]\n",
        "\n",
        "        embedding = attention_pooling(hidden_states)  # [hidden_size]\n",
        "\n",
        "        if l2_normalize:\n",
        "            embedding = embedding / embedding.norm(p=2)\n",
        "\n",
        "    return embedding\n",
        "\n",
        "import torch\n",
        "\n",
        "def augment_embedding(emb: torch.Tensor, noise_level: float = 0.01) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns a slightly perturbed version of the embedding.\n",
        "    Small Gaussian noise is added to simulate variation in speech.\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(emb) * noise_level\n",
        "    return emb + noise\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import random\n",
        "import gc\n",
        "from typing import List, Dict\n",
        "\n",
        "# ---------- Utility ----------\n",
        "def _ensure_ct(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Ensure shape is (channels, time).\"\"\"\n",
        "    x = x.squeeze()\n",
        "    if x.dim() == 1:\n",
        "        x = x.unsqueeze(0)\n",
        "    elif x.dim() > 2:\n",
        "        new_c = int(torch.prod(torch.tensor(x.shape[:-1])).item())\n",
        "        x = x.reshape(new_c, x.shape[-1])\n",
        "    return x.contiguous()\n",
        "\n",
        "def _ensure_mono(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Convert (channels, time) -> (time,) mono.\"\"\"\n",
        "    x = _ensure_ct(x)\n",
        "    if x.shape[0] > 1:\n",
        "        x = x.mean(dim=0)\n",
        "    else:\n",
        "        x = x.squeeze(0)\n",
        "    return x.contiguous()\n",
        "\n",
        "def _ensure_bct(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Convert (channels, time) -> (batch=1, channels, time).\"\"\"\n",
        "    x = _ensure_ct(x)\n",
        "    return x.unsqueeze(0).contiguous()\n",
        "\n",
        "# ---------- Chunked transform ----------\n",
        "def apply_in_chunks(waveform: torch.Tensor, transform, chunk_size: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Apply a torchaudio transform in memory-safe chunks.\n",
        "    Input/output are (channels, time).\n",
        "    \"\"\"\n",
        "    wf = _ensure_ct(waveform)\n",
        "    num_samples = wf.shape[1]\n",
        "    if num_samples == 0:\n",
        "        return wf\n",
        "\n",
        "    chunk_size = max(1, int(chunk_size))\n",
        "    chunks = []\n",
        "\n",
        "    with torch.no_grad():  # prevent computation graph buildup\n",
        "        for start in range(0, num_samples, chunk_size):\n",
        "            end = min(start + chunk_size, num_samples)\n",
        "            chunk = wf[:, start:end].contiguous()\n",
        "\n",
        "            out = transform(chunk)\n",
        "            out = _ensure_ct(out)\n",
        "            chunks.append(out)\n",
        "\n",
        "            del chunk, out\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    return torch.cat(chunks, dim=1)\n",
        "\n",
        "# ---------- Augmentation ----------\n",
        "def create_audio_variations(\n",
        "    waveform: torch.Tensor,\n",
        "    sample_rate: int,\n",
        "    num_variations: int = 2,\n",
        "    max_chunk_seconds: float = 1.0\n",
        ") -> List[torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Returns a list of augmented mono waveforms (1D) ready for embedding.\n",
        "    Each tensor shape: [T]\n",
        "    \"\"\"\n",
        "    base = _ensure_ct(waveform).cpu()\n",
        "    chunk_size = max(1, int(sample_rate * max_chunk_seconds))\n",
        "    variations = []\n",
        "\n",
        "    for i in range(num_variations):\n",
        "        aug = base.clone().detach()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Pitch shift\n",
        "            if random.random() < 0.5:\n",
        "                n_steps = random.uniform(-2.0, 2.0)\n",
        "                print(f\"  Pitch: {n_steps:+.2f} semitones\")\n",
        "                pitch_shift = T.PitchShift(sample_rate, n_steps=n_steps)\n",
        "                aug = apply_in_chunks(aug, pitch_shift, chunk_size)\n",
        "                del pitch_shift\n",
        "                gc.collect()\n",
        "\n",
        "            # Noise\n",
        "            if random.random() < 0.5:\n",
        "                sigma = random.uniform(0.001, 0.01)\n",
        "                print(f\"  Noise sigma={sigma:.4f}\")\n",
        "                aug.add_(torch.randn_like(aug) * sigma)\n",
        "                gc.collect()\n",
        "\n",
        "            # Gain\n",
        "            if random.random() < 0.5:\n",
        "                gain_db = random.uniform(-2.0, 2.0)\n",
        "                print(f\"  Gain: {gain_db:+.2f} dB\")\n",
        "                aug.mul_(10.0 ** (gain_db / 20.0))\n",
        "\n",
        "            # Speed change\n",
        "            if random.random() < 0.5:\n",
        "                speed = random.uniform(0.95, 1.05)\n",
        "                print(f\"  Speed factor: {speed:.3f}\")\n",
        "                new_sr = max(1, int(sample_rate * speed))\n",
        "                resample_up = T.Resample(sample_rate, new_sr)\n",
        "                resample_down = T.Resample(new_sr, sample_rate)\n",
        "                aug = apply_in_chunks(aug, resample_up, chunk_size/2)\n",
        "                aug = apply_in_chunks(aug, resample_down, chunk_size/2)\n",
        "                del resample_up, resample_down\n",
        "                gc.collect()\n",
        "\n",
        "        # Convert to mono 1D [T]\n",
        "        mono_aug = _ensure_mono(aug)\n",
        "        variations.append(mono_aug)\n",
        "\n",
        "        del aug\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return variations\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import io\n",
        "import base64\n",
        "\n",
        "def audio_to_base64(waveform: torch.Tensor, sample_rate: int) -> str:\n",
        "    \"\"\"\n",
        "    Convert a waveform tensor to WAV and encode as Base64.\n",
        "    \"\"\"\n",
        "    # Detach and move to CPU\n",
        "    wav = waveform.detach().cpu()\n",
        "    wav_np = wav.numpy()\n",
        "\n",
        "    # If multi-channel, make sure shape is (N,) or (N, C)\n",
        "    if wav_np.ndim > 1:\n",
        "        wav_np = wav_np.T  # scipy expects shape (N, C)\n",
        "\n",
        "    # Convert to float32 if needed\n",
        "    if not np.issubdtype(wav_np.dtype, np.floating):\n",
        "        wav_np = wav_np.astype(np.float32)\n",
        "\n",
        "    # Write to in-memory WAV file\n",
        "    buf = io.BytesIO()\n",
        "    wavfile.write(buf, sample_rate, wav_np)\n",
        "    buf.seek(0)\n",
        "    b64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
        "    return b64\n",
        "\n",
        "from typing import List, Dict\n",
        "from google.colab import files\n",
        "import torch\n",
        "\n",
        "# ---------- Reference index builder ----------\n",
        "def build_reference_index_from_list(\n",
        "    intents,\n",
        "    samples_per_intent: int = 3,\n",
        "    augment: bool = True,\n",
        "    augment_factor: int = 3\n",
        ") -> Dict[str, Dict[str, List[torch.Tensor]]]:\n",
        "    \"\"\"\n",
        "    Builds reference index with original + augmented embeddings.\n",
        "    Processes each file individually to minimize memory footprint.\n",
        "    \"\"\"\n",
        "    reference_index: Dict[str, Dict[str, List[torch.Tensor]]] = {}\n",
        "\n",
        "    for intent in intents:\n",
        "        print(f\"\\nPlease upload {samples_per_intent} audio samples for intent: '{intent}'\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        emb_list: List[torch.Tensor] = []\n",
        "        individual_list = []\n",
        "        original_base64_list = []\n",
        "\n",
        "        for fname in uploaded.keys():\n",
        "            wav = load_and_preprocess(fname, remove_silence=False)  # should return mono or stereo tensor\n",
        "            wav_mono = _ensure_mono(wav)  # ensure mono before embedding\n",
        "\n",
        "            # Original embedding\n",
        "            emb = audio_to_embedding(wav_mono.detach(), l2_normalize=True)\n",
        "            emb_base64 = audio_to_base64(wav_mono.detach(), sample_rate=TARGET_SR)\n",
        "            individual_list.append({\"embedding\": emb, \"audio_base64\": emb_base64})\n",
        "            original_base64_list.append(emb_base64)\n",
        "            del wav, emb\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Augmentation\n",
        "            if augment:\n",
        "                print(f\"Creating variations for '{fname}'...\")\n",
        "                variations = create_audio_variations(\n",
        "                    wav_mono,\n",
        "                    sample_rate=TARGET_SR,\n",
        "                    num_variations=augment_factor,\n",
        "                    max_chunk_seconds=1.0\n",
        "                )\n",
        "                for i, var_wav in enumerate(variations, start=1):\n",
        "                    aug_emb = audio_to_embedding(var_wav.detach(), l2_normalize=True)\n",
        "                    aug_base64 = audio_to_base64(var_wav.detach(), sample_rate=TARGET_SR)\n",
        "                    individual_list.append({\"embedding\": aug_emb, \"audio_base64\": aug_base64})\n",
        "                    del var_wav, aug_emb\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    print(f\"Processed variation {i}.\")\n",
        "\n",
        "        # Average embedding from originals\n",
        "        avg_emb = average_embeddings(\n",
        "            [d[\"embedding\"] for d in individual_list[:samples_per_intent]],\n",
        "            l2_normalize=True\n",
        "        )\n",
        "\n",
        "        reference_index[intent] = {\n",
        "            \"individual\": individual_list,\n",
        "            \"average\": {\"embedding\": avg_emb, \"audio_base64_list\": original_base64_list}\n",
        "        }\n",
        "\n",
        "        print(f\"Intent '{intent}': {len(individual_list)} embeddings stored.\")\n",
        "\n",
        "    print(\"\\n✅ Reference index ready!\")\n",
        "    return reference_index\n",
        "\n",
        "# Call it to start collecting audio samples\n",
        "\n",
        "# Predefined intents/phrases (expand this to 40–50)\n",
        "COMMON_INTENTS = [\n",
        "    \"Thank you\",\n",
        "    \"I need help\",\n",
        "    \"I need Water\"\n",
        "    # … add all the rest\n",
        "]\n",
        "\n",
        "\n",
        "# qdrant Part\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "import torch\n",
        "\n",
        "def save_reference_index_to_qdrant(\n",
        "    reference_index: dict,\n",
        "    username: str,\n",
        "    collection_name: str = \"speech_intents_for_attention_polling\",\n",
        "    qdrant_url: str = \"https://7d3e9db7-bab2-4f55-952c-fde18ffb7d98.eu-west-1-0.aws.cloud.qdrant.io\",\n",
        "    api_key: str = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.vWJNPK2iJIZoYAsBuuqD03ZvkxhKAhaDaB9UujOPv3s\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Saves both individual and average embeddings for each intent to Qdrant.\n",
        "    \"\"\"\n",
        "\n",
        "    client = QdrantClient(url=qdrant_url, api_key=api_key)\n",
        "\n",
        "    # Ensure collection exists\n",
        "    first_intent = next(iter(reference_index.values()))\n",
        "    first_vector = first_intent[\"average\"][\"embedding\"]\n",
        "    vector_size = first_vector.shape[0]\n",
        "\n",
        "    if not client.collection_exists(collection_name):\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE),\n",
        "        )\n",
        "\n",
        "    payloads = []\n",
        "    vectors = []\n",
        "\n",
        "    for intent, data in reference_index.items():\n",
        "        # Save individual vectors\n",
        "        for item in data[\"individual\"]:\n",
        "            vec = item[\"embedding\"]\n",
        "            audio_b64 = item[\"audio_base64\"]\n",
        "            vector_list = vec.detach().cpu().tolist()\n",
        "            payloads.append({\"username\": username, \"intent\": intent, \"type\": \"individual\",\"audio_b64\": None})\n",
        "            vectors.append(vector_list)\n",
        "\n",
        "        # Save average vector\n",
        "\n",
        "        avg_vec = data[\"average\"][\"embedding\"]\n",
        "        avg_vec_list = avg_vec.detach().cpu().tolist()\n",
        "        payloads.append({\"username\": username, \"intent\": intent, \"type\": \"average\",\"audio_b64\": None})\n",
        "        vectors.append(avg_vec_list)\n",
        "\n",
        "    # Insert into Qdrant\n",
        "    client.upload_collection(\n",
        "        collection_name=collection_name,\n",
        "        payload=payloads,\n",
        "        vectors=vectors,\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Saved {len(vectors)} vectors for user '{username}' into Qdrant.\")\n",
        "\n",
        "\n",
        "\"\"\"You only need to run this once for the collection.\"\"\"\n",
        "\n",
        "# You only need to run this once for the collection.\n",
        "# This tells Qdrant to treat username as a keyword and allows filtering on it efficiently.\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "\n",
        "from qdrant_client.http import models as rest\n",
        "\n",
        "qdrant_url = \"https://7d3e9db7-bab2-4f55-952c-fde18ffb7d98.eu-west-1-0.aws.cloud.qdrant.io\"\n",
        "api_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.vWJNPK2iJIZoYAsBuuqD03ZvkxhKAhaDaB9UujOPv3s\"\n",
        "\n",
        "client = QdrantClient(url=qdrant_url, api_key=api_key)\n",
        "\n",
        "# Create index for 'username' as a keyword\n",
        "client.create_payload_index(\n",
        "    collection_name=\"speech_intents_for_attention_polling\",\n",
        "    field_name=\"username\",\n",
        "    field_schema=rest.PayloadSchemaType.KEYWORD\n",
        ")\n",
        "\n",
        "# Create index for 'type' as a keyword\n",
        "client.create_payload_index(\n",
        "    collection_name=\"speech_intents_for_attention_polling\",\n",
        "    field_name=\"type\",\n",
        "    field_schema=rest.PayloadSchemaType.KEYWORD\n",
        ")\n",
        "\n",
        "\"\"\"Fetch Vector Data From Qdrant for given user\"\"\"\n",
        "\n",
        "def classify_audio(embedding, username, top_k=5, confidence_threshold=0.6, avg_weight=1.5):\n",
        "\n",
        "    qdrant_url: str = \"https://7d3e9db7-bab2-4f55-952c-fde18ffb7d98.eu-west-1-0.aws.cloud.qdrant.io\"\n",
        "    api_key: str = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.vWJNPK2iJIZoYAsBuuqD03ZvkxhKAhaDaB9UujOPv3s\"\n",
        "    client = QdrantClient(url=qdrant_url, api_key=api_key)\n",
        "\n",
        "    # Query individual vectors first\n",
        "    search_results = client.query_points(\n",
        "        collection_name=\"speech_intents_for_attention_polling\",\n",
        "        query=embedding.detach().cpu().tolist(),\n",
        "        limit=top_k,\n",
        "        query_filter=models.Filter(\n",
        "            must=[\n",
        "                models.FieldCondition(key=\"username\", match=models.MatchValue(value=username)),\n",
        "                models.FieldCondition(key=\"type\", match=models.MatchValue(value=\"individual\"))\n",
        "            ]\n",
        "        )\n",
        "    ).points\n",
        "\n",
        "    if not search_results:\n",
        "        return None, 0.0\n",
        "\n",
        "    # Weighted voting for individual vectors\n",
        "    intent_scores = {}\n",
        "    for hit in search_results:\n",
        "        lbl = hit.payload[\"intent\"]\n",
        "        score = hit.score\n",
        "        intent_scores[lbl] = intent_scores.get(lbl, 0) + score\n",
        "        scores = [hit.score for hit in search_results]  # cosine similarity score\n",
        "\n",
        "    predicted_intent = max(intent_scores, key=intent_scores.get)\n",
        "    confidence = intent_scores[predicted_intent] /  sum(intent_scores.values())  # sum(scores)\n",
        "\n",
        "    # Fallback to average vectors if confidence is low\n",
        "    if confidence < confidence_threshold:\n",
        "        avg_results = client.query_points(\n",
        "            collection_name=\"speech_intents_for_attention_polling\",\n",
        "            query=embedding.detach().cpu().tolist(),\n",
        "            limit=top_k,\n",
        "            query_filter=models.Filter(\n",
        "                must=[\n",
        "                    models.FieldCondition(key=\"username\", match=models.MatchValue(value=username)),\n",
        "                    models.FieldCondition(key=\"type\", match=models.MatchValue(value=\"average\"))\n",
        "                ]\n",
        "            )\n",
        "        ).points\n",
        "\n",
        "        if avg_results:\n",
        "            intent_scores = {}\n",
        "            for hit in avg_results:\n",
        "                lbl = hit.payload[\"intent\"]\n",
        "                score = hit.score * avg_weight\n",
        "                intent_scores[lbl] = intent_scores.get(lbl, 0) + score\n",
        "\n",
        "            predicted_intent = max(intent_scores, key=intent_scores.get)\n",
        "            confidence = intent_scores[predicted_intent] / sum([h.score for h in avg_results])\n",
        "\n",
        "    return predicted_intent, confidence\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create reference_index"
      ],
      "metadata": {
        "id": "CppRUfuKasEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#create reference_index\n",
        "\n",
        "reference_index = build_reference_index_from_list(\n",
        "    intents=COMMON_INTENTS,\n",
        "    samples_per_intent=3  # or 2, depending on your choice\n",
        ")\n",
        "\n",
        "\n",
        "# Loop over each item in the reference_index dictionary\n",
        "for intent, ref in reference_index.items():\n",
        "    print(f\"Intent: {intent}\")\n",
        "\n",
        "    # Average embedding info\n",
        "    avg_emb = ref['average']['embedding']\n",
        "    avg_audio_list = ref['average']['audio_base64_list']\n",
        "    print(f\"Average embedding shape: {avg_emb.shape}\")\n",
        "    print(f\"Average embedding as list: {avg_emb.tolist()}\")\n",
        "    print(f\"Number of original audio Base64 clips: {len(avg_audio_list)}\")\n",
        "\n",
        "    # Individual embeddings info\n",
        "    for i, item in enumerate(ref['individual'], start=1):\n",
        "        emb = item['embedding']\n",
        "        audio_base64 = item['audio_base64']\n",
        "        # print(f\"Individual #{i} embedding shape: {emb.shape}\")\n",
        "        print(f\"Individual #{i} embedding as list: {emb.tolist()}\")\n",
        "        print(f\"Individual #{i} audio Base64 length: {len(audio_base64)}\")\n",
        "\n",
        "    print(\"-\" * 40)  # Separator for clarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "j8mh75GGaUCr",
        "outputId": "b72ae7bb-7a7f-43f6-a24c-9acd5a5e0db0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Please upload 3 audio samples for intent: 'Thank you'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-583668e4-cacd-4515-ad7d-e5c8d063f67f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-583668e4-cacd-4515-ad7d-e5c8d063f67f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Thank you 1.mp3 to Thank you 1 (1).mp3\n",
            "Creating variations for 'Thank you 1 (1).mp3'...\n",
            "  Pitch: -0.43 semitones\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1644100482.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#create reference_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m reference_index = build_reference_index_from_list(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mintents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOMMON_INTENTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msamples_per_intent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m  \u001b[0;31m# or 2, depending on your choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-252436543.py\u001b[0m in \u001b[0;36mbuild_reference_index_from_list\u001b[0;34m(intents, samples_per_intent, augment, augment_factor)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating variations for '{fname}'...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 variations = create_audio_variations(\n\u001b[0m\u001b[1;32m    310\u001b[0m                     \u001b[0mwav_mono\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_SR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-252436543.py\u001b[0m in \u001b[0;36mcreate_audio_variations\u001b[0;34m(waveform, sample_rate, num_variations, max_chunk_seconds)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Pitch: {n_steps:+.2f} semitones\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mpitch_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPitchShift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0maug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch_shift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mpitch_shift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-252436543.py\u001b[0m in \u001b[0;36mapply_in_chunks\u001b[0;34m(waveform, transform, chunk_size)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_ct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1880\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1804\u001b[0m                 ):\n\u001b[1;32m   1805\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks_with_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m                         \u001b[0margs_kwargs_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0margs_kwargs_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_kwargs_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_kwargs_result\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/lazy.py\u001b[0m in \u001b[0;36m_infer_parameters\u001b[0;34m(self, module, args, kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \"\"\"\n\u001b[1;32m    260\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_uninitialized_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'module {self._get_name()} has not been fully initialized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/transforms/_transforms.py\u001b[0m in \u001b[0;36minitialize_parameters\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_freq\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1734\u001b[0;31m                     kernel, self.width = _get_sinc_resample_kernel(\n\u001b[0m\u001b[1;32m   1735\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1964\u001b[0m         )\n\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1966\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1967\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts_or_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts_or_sets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save to Qdrant"
      ],
      "metadata": {
        "id": "35yPiPkPa4ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: save to Qdrant\n",
        "save_reference_index_to_qdrant(reference_index, username=\"dimple\")"
      ],
      "metadata": {
        "id": "le2FSDcRa3fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classify a new audio clip"
      ],
      "metadata": {
        "id": "h4HrPRZcaxCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Classify a new audio clip ===\")\n",
        "print(\"Please upload the audio file you want to classify…\")\n",
        "\n",
        "uploaded = files.upload()  # User uploads file\n",
        "fname = next(iter(uploaded.keys()))\n",
        "\n",
        "# Convert to embedding\n",
        "wav = load_and_preprocess(fname, remove_silence=False)\n",
        "emb = audio_to_embedding(wav, l2_normalize=True)\n",
        "\n",
        "\n",
        "# Classify via Qdrant\n",
        "predicted, conf = classify_audio(emb, username=\"dimple\")\n",
        "\n",
        "if predicted:\n",
        "    print(f\"\\n✅ Predicted Intent: {predicted}  |  Confidence: {conf:.2f}\")\n",
        "else:\n",
        "    print(f\"\\n⚠️ No confident match found. Confidence: {conf:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "Kxmgv35xaVwz",
        "outputId": "0e2a55aa-9a17-44ea-c6f8-334a5b13c5b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Classify a new audio clip ===\n",
            "Please upload the audio file you want to classify…\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8dc4f1af-9b82-4580-b231-d953f9d213d9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8dc4f1af-9b82-4580-b231-d953f9d213d9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving I need Help 1.mp3 to I need Help 1.mp3\n",
            "\n",
            "✅ Predicted Intent: I need help  |  Confidence: 0.60\n"
          ]
        }
      ]
    }
  ]
}