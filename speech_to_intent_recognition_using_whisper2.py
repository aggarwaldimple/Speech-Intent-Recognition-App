# -*- coding: utf-8 -*-
"""Speech_to_intent_recognition_using_whisper2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jKx9KeZ4LLAXBFhQvYgIyGG7uGz_Qx5i
"""

!pip install torchaudio pydub scikit-learn numpy

!pip install transformers==4.41.1 --upgrade --quiet


!pip install -U openai-whisper
!sudo apt update && sudo apt install ffmpeg

!pip install qdrant-client



from typing import Dict, List, Tuple

import numpy as np
import io
import os

from google.colab import files

import torch
import torchaudio
import torchaudio.transforms as T
import random
import gc



# Audio utils
from pydub import AudioSegment
from pydub.silence import detect_nonsilent

# openai/whisper-medium

from transformers import WhisperProcessor, WhisperModel

MODEL_NAME = "openai/whisper-medium"
processor = WhisperProcessor.from_pretrained(MODEL_NAME)
model = WhisperModel.from_pretrained(MODEL_NAME)
model.eval()

# Qdrant

from qdrant_client import QdrantClient
from qdrant_client.http import models

# -----------------------------
# Config
# -----------------------------
TARGET_SR = 16000  # Wav2Vec2 expects 16 kHz
MIN_SILENCE_LEN_MS = 150  # consecutive ms of silence to consider as silence
SILENCE_DB_OFFSET = 16    # silence threshold = (audio.dBFS - this)

# -----------------------------
# Audio preprocessing
# -----------------------------

def load_and_preprocess(path: str,
                        target_sr: int = TARGET_SR,
                        remove_silence: bool = False) -> torch.Tensor:
    """
    Load an audio file of any common type (mp3/m4a/wav),
    convert to mono 16 kHz, optionally remove silence,
    and return a 1-D float32 waveform tensor in [-1, 1].
    """
    # 1) Load with pydub (handles many formats)
    audio = AudioSegment.from_file(path)

    # 2) Standardize sample rate + channels
    audio = audio.set_frame_rate(target_sr).set_channels(1)

    # 3) Remove silence using energy thresholding
    if remove_silence:
        # threshold below which audio is considered silent
        silence_thresh = audio.dBFS - SILENCE_DB_OFFSET
        ranges = detect_nonsilent(audio, min_silence_len=MIN_SILENCE_LEN_MS,
                                  silence_thresh=silence_thresh)
        if ranges:
            trimmed = AudioSegment.silent(duration=0, frame_rate=target_sr)
            for start_ms, end_ms in ranges:
                trimmed += audio[start_ms:end_ms]
            audio = trimmed
        # if no ranges found, keep original audio

    # 4) Convert to numpy float32 in [-1, 1]
    samples = np.array(audio.get_array_of_samples())
    # Scale based on sample width (e.g., 16-bit -> 32768)
    max_val = float(1 << (8 * audio.sample_width - 1))
    samples = (samples.astype(np.float32) / max_val)

    # 5) To torch tensor, shape [time]; keep as 1-D for HF extractor
    waveform = torch.from_numpy(samples)
    return waveform

# -----------------------------
# Average Embeddings
# -----------------------------

def average_embeddings(emb_list: List[torch.Tensor], l2_normalize: bool = False) -> torch.Tensor:
    """Average multiple 1024-d embeddings and (optionally) L2-normalize the result."""
    if len(emb_list) == 1:
        avg = emb_list[0]
    else:
        avg = torch.stack(emb_list, dim=0).mean(dim=0)
    if l2_normalize:
        avg = avg / avg.norm(p=2).clamp_min(1e-12)
    return avg


# ---------- Utility ----------
def _ensure_ct(x: torch.Tensor) -> torch.Tensor:
    """Ensure shape is (channels, time)."""
    x = x.squeeze()
    if x.dim() == 1:
        x = x.unsqueeze(0)
    elif x.dim() > 2:
        new_c = int(torch.prod(torch.tensor(x.shape[:-1])).item())
        x = x.reshape(new_c, x.shape[-1])
    return x.contiguous()

def _ensure_mono(x: torch.Tensor) -> torch.Tensor:
    """Convert (channels, time) -> (time,) mono."""
    x = _ensure_ct(x)
    if x.shape[0] > 1:
        x = x.mean(dim=0)
    else:
        x = x.squeeze(0)
    return x.contiguous()


# ---------- Chunked transform ----------
def apply_in_chunks(waveform: torch.Tensor, transform, chunk_size: int) -> torch.Tensor:
    """
    Apply a torchaudio transform in memory-safe chunks.
    Input/output are (channels, time).
    """
    wf = _ensure_ct(waveform)
    num_samples = wf.shape[1]
    if num_samples == 0:
        return wf

    chunk_size = max(1, int(chunk_size))
    chunks = []

    with torch.no_grad():  # prevent computation graph buildup
        for start in range(0, num_samples, chunk_size):
            end = min(start + chunk_size, num_samples)
            chunk = wf[:, start:end].contiguous()

            out = transform(chunk)
            out = _ensure_ct(out)
            chunks.append(out)

            del chunk, out
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    return torch.cat(chunks, dim=1)

# ---------- Augmentation ----------
def create_audio_variations(
    waveform: torch.Tensor,
    sample_rate: int,
    num_variations: int = 2,
    max_chunk_seconds: float = 1.0
) -> List[torch.Tensor]:
    """
    Returns a list of augmented mono waveforms (1D) ready for embedding.
    Each tensor shape: [T]
    """
    base = _ensure_ct(waveform).cpu()
    chunk_size = max(1, int(sample_rate * max_chunk_seconds))
    variations = []

    for i in range(num_variations):
        aug = base.clone().detach()

        with torch.no_grad():
            # Pitch shift
            if random.random() < 0.5:
                n_steps = random.uniform(-2.0, 2.0)
                print(f"  Pitch: {n_steps:+.2f} semitones")
                pitch_shift = T.PitchShift(sample_rate, n_steps=n_steps)
                aug = apply_in_chunks(aug, pitch_shift, chunk_size)
                del pitch_shift
                gc.collect()

            # Noise
            if random.random() < 0.5:
                sigma = random.uniform(0.001, 0.01)
                print(f"  Noise sigma={sigma:.4f}")
                aug.add_(torch.randn_like(aug) * sigma)
                gc.collect()

            # Gain
            if random.random() < 0.5:
                gain_db = random.uniform(-2.0, 2.0)
                print(f"  Gain: {gain_db:+.2f} dB")
                aug.mul_(10.0 ** (gain_db / 20.0))

            # Speed change
            if random.random() < 0.5:
                speed = random.uniform(0.95, 1.05)
                print(f"  Speed factor: {speed:.3f}")
                new_sr = max(1, int(sample_rate * speed))
                resample_up = T.Resample(sample_rate, new_sr)
                resample_down = T.Resample(new_sr, sample_rate)
                aug = apply_in_chunks(aug, resample_up, chunk_size/2)
                aug = apply_in_chunks(aug, resample_down, chunk_size/2)
                del resample_up, resample_down
                gc.collect()

        # Convert to mono 1D [T]
        mono_aug = _ensure_mono(aug)
        variations.append(mono_aug)

        del aug
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    return variations


# Same AttentionPooling as before
class AttentionPoolings(torch.nn.Module):
    def __init__(self, hidden_size: int):
        super().__init__()
        self.attention = torch.nn.Linear(hidden_size, 1)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # hidden_states: [seq_len, hidden_size]
        attn_scores = self.attention(hidden_states)           # [seq_len, 1]
        attn_weights = torch.softmax(attn_scores, dim=0)      # [seq_len, 1]
        pooled = torch.sum(attn_weights * hidden_states, dim=0)  # [hidden_size]
        return pooled


# Whisper embedding function with attention pooling
def audio_to_embedding_using_Whisper(wav: torch.Tensor, l2_normalize: bool = True) -> torch.Tensor:
    """
    Converts waveform to embedding using Whisper encoder.
    Applies attention pooling over last hidden layer.
    """
    if wav.ndim == 2 and wav.shape[0] > 1:
        wav = wav.mean(dim=0)  # stereo to mono

    wav = wav.float().squeeze()

    # Preprocess
    inputs = processor(wav.numpy(), sampling_rate=16000, return_tensors="pt")

    with torch.no_grad():
        outputs = model.encoder(inputs.input_features, output_hidden_states=True)
        hidden_states = outputs.hidden_states[-1]  # [batch, seq_len, hidden_size]

    # Remove batch dimension
    hidden_states = hidden_states.squeeze(0)  # [seq_len, hidden_size]

    # Apply attention pooling
    attn_pool = AttentionPoolings(hidden_states.size(-1))
    embedding = attn_pool(hidden_states)  # [hidden_size]

    if l2_normalize:
        embedding = embedding / embedding.norm(p=2)

    return embedding


# ---------- Reference index builder ----------
def build_reference_index_from_list(
    intents,
    samples_per_intent: int = 3,
    augment: bool = True,
    augment_factor: int = 2
) -> Dict[str, Dict[str, List[torch.Tensor]]]:
    """
    Builds reference index with original + augmented embeddings.
    Processes each file individually to minimize memory footprint.
    """
    reference_index: Dict[str, Dict[str, List[torch.Tensor]]] = {}

    for intent in intents:
        print(f"\nPlease upload {samples_per_intent} audio samples for intent: '{intent}'")
        uploaded = files.upload()

        emb_list: List[torch.Tensor] = []
        individual_list = []
        original_base64_list = []

        for fname in uploaded.keys():
            wav = load_and_preprocess(fname, remove_silence=False)  # should return mono or stereo tensor
            wav_mono = _ensure_mono(wav)  # ensure mono before embedding

            # Original embedding
            print("Creating Vector Embedding")
            emb = audio_to_embedding_using_Whisper(wav_mono.detach(), l2_normalize=True)
            print("Created Vector Embedding")
            individual_list.append({"embedding": emb, "audio_base64": None})
            del wav, emb
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # Augmentation
            if augment:
                print(f"Creating variations for '{fname}'...")
                variations = create_audio_variations(
                    wav_mono,
                    sample_rate=TARGET_SR,
                    num_variations=augment_factor,
                    max_chunk_seconds=1.0
                )
                for i, var_wav in enumerate(variations, start=1):
                    print("Creating Vector Embedding")
                    aug_emb = audio_to_embedding_using_Whisper(var_wav.detach(), l2_normalize=True)
                    print("Created Vector Embedding")
                    individual_list.append({"embedding": aug_emb, "audio_base64": None})
                    del var_wav, aug_emb
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    print(f"Processed variation {i}.")

        # Average embedding from originals
        avg_emb = average_embeddings(
            [d["embedding"] for d in individual_list[:samples_per_intent]],
            l2_normalize=True
        )

        reference_index[intent] = {
            "individual": individual_list,
            "average": {"embedding": avg_emb, "audio_base64_list": None}
        }

        print(f"Intent '{intent}': {len(individual_list)} embeddings stored.")

    print("\n✅ Reference index ready!")
    return reference_index

# qdrant Part

def save_reference_index_to_qdrant(
    reference_index: dict,
    username: str,
    collection_name: str = "speech_intents_for_attention_polling_using_whisper2",
    qdrant_url: str = "https://7d3e9db7-bab2-4f55-952c-fde18ffb7d98.eu-west-1-0.aws.cloud.qdrant.io",
    api_key: str = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.vWJNPK2iJIZoYAsBuuqD03ZvkxhKAhaDaB9UujOPv3s"
):
    """
    Saves both individual and average embeddings for each intent to Qdrant.
    """

    client = QdrantClient(url=qdrant_url, api_key=api_key)

    # Ensure collection exists
    first_intent = next(iter(reference_index.values()))
    first_vector = first_intent["average"]["embedding"]
    vector_size = first_vector.shape[0]

    if not client.collection_exists(collection_name):
        client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE),
        )

    payloads = []
    vectors = []

    for intent, data in reference_index.items():
        # Save individual vectors
        for item in data["individual"]:
            vec = item["embedding"]
            audio_b64 = item["audio_base64"]
            vector_list = vec.detach().cpu().tolist()
            payloads.append({"username": username, "intent": intent, "type": "individual","audio_b64": None})
            vectors.append(vector_list)

        # Save average vector

        avg_vec = data["average"]["embedding"]
        avg_vec_list = avg_vec.detach().cpu().tolist()
        payloads.append({"username": username, "intent": intent, "type": "average","audio_b64": None})
        vectors.append(avg_vec_list)

    # Insert into Qdrant
    client.upload_collection(
        collection_name=collection_name,
        payload=payloads,
        vectors=vectors,
    )

    print(f"✅ Saved {len(vectors)} vectors for user '{username}' into Qdrant.")



"""Fetch Vector Data From Qdrant for given user"""

def classify_audio(embedding, username, top_k=5, confidence_threshold=0.6, avg_weight=1.5):

    qdrant_url: str = "https://7d3e9db7-bab2-4f55-952c-fde18ffb7d98.eu-west-1-0.aws.cloud.qdrant.io"
    api_key: str = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.vWJNPK2iJIZoYAsBuuqD03ZvkxhKAhaDaB9UujOPv3s"
    client = QdrantClient(url=qdrant_url, api_key=api_key)

    # Query individual vectors first
    search_results = client.query_points(
        collection_name="speech_intents_for_attention_polling_using_whisper2",
        query=embedding.detach().cpu().tolist(),
        limit=top_k,
        query_filter=models.Filter(
            must=[
                models.FieldCondition(key="username", match=models.MatchValue(value=username)),
                models.FieldCondition(key="type", match=models.MatchValue(value="individual"))
            ]
        )
    ).points

    if not search_results:
        return None, 0.0

    # Weighted voting for individual vectors
    intent_scores = {}
    for hit in search_results:
        lbl = hit.payload["intent"]
        score = hit.score
        intent_scores[lbl] = intent_scores.get(lbl, 0) + score

    scores = [hit.score for hit in search_results]  # cosine similarity score
    predicted_intent = max(intent_scores, key=intent_scores.get)
    confidence = intent_scores[predicted_intent] /  sum(intent_scores.values())  # sum(scores)

    # Fallback to average vectors if confidence is low
    if confidence < confidence_threshold:
        avg_results = client.query_points(
            collection_name="speech_intents_for_attention_polling_using_whisper2",
            query=embedding.detach().cpu().tolist(),
            limit=top_k,
            query_filter=models.Filter(
                must=[
                    models.FieldCondition(key="username", match=models.MatchValue(value=username)),
                    models.FieldCondition(key="type", match=models.MatchValue(value="average"))
                ]
            )
        ).points

        if avg_results:
            intent_scores = {}
            for hit in avg_results:
                lbl = hit.payload["intent"]
                score = hit.score * avg_weight
                intent_scores[lbl] = intent_scores.get(lbl, 0) + score

            predicted_intent = max(intent_scores, key=intent_scores.get)
            confidence = intent_scores[predicted_intent] / sum([h.score for h in avg_results])

    return predicted_intent, confidence, intent_scores

# Predefined intents/phrases (expand this to 40–50)
COMMON_INTENTS = [
    # "Thank you",
    "I need help",
    # "I need Water"
    # … add all the rest
]


reference_index = build_reference_index_from_list(
    intents=COMMON_INTENTS,
    samples_per_intent=3  # or 2, depending on your choice
)

# Loop over each item in the reference_index dictionary
for intent, ref in reference_index.items():
    print(f"Intent: {intent}")

    # Average embedding info
    avg_emb = ref['average']['embedding']
    avg_audio_list = ref['average']['audio_base64_list']
    print(f"Average embedding shape: {avg_emb.shape}")
    print(f"Average embedding as list: {avg_emb.tolist()}")

    # Individual embeddings info
    for i, item in enumerate(ref['individual'], start=1):
        emb = item['embedding']
        # print(f"Individual #{i} embedding as list: {emb.tolist()}")

    print("-" * 40)  # Separator for clarity

# Step 2: save to Qdrant
save_reference_index_to_qdrant(reference_index, username="dimple")

# You only need to run this once for the collection.
# This tells Qdrant to treat username as a keyword and allows filtering on it efficiently.

from qdrant_client import QdrantClient
from qdrant_client.http import models

from qdrant_client.http import models as rest

qdrant_url = "https://7d3e9db7-bab2-4f55-952c-fde18ffb7d98.eu-west-1-0.aws.cloud.qdrant.io"
api_key = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.vWJNPK2iJIZoYAsBuuqD03ZvkxhKAhaDaB9UujOPv3s"

client = QdrantClient(url=qdrant_url, api_key=api_key)

# Create index for 'username' as a keyword
client.create_payload_index(
    collection_name="speech_intents_for_attention_polling_using_whisper2",
    field_name="username",
    field_schema=rest.PayloadSchemaType.KEYWORD
)

# Create index for 'type' as a keyword
client.create_payload_index(
    collection_name="speech_intents_for_attention_polling_using_whisper2",
    field_name="type",
    field_schema=rest.PayloadSchemaType.KEYWORD
)

# === Example usage ===
print("\n=== Classify a new audio clip ===")
print("Please upload the audio file you want to classify…")

uploaded = files.upload()  # User uploads file
fname = next(iter(uploaded.keys()))

# Convert to embedding
wav = load_and_preprocess(fname, remove_silence=False)
emb = audio_to_embedding_using_Whisper(wav, l2_normalize=True)


# Classify via Qdrant
predicted, conf, intent_scores = classify_audio(emb, username="dimple")

if predicted:
    print(f"\n✅ Predicted Intent: {predicted}  |  Confidence: {conf:.2f}")
else:
    print(f"\n⚠️ No confident match found. Confidence: {conf:.2f}")